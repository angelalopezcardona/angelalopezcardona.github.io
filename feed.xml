<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://angelalopezcardona.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://angelalopezcardona.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-20T15:47:21+00:00</updated><id>https://angelalopezcardona.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Aligning Tokens between different LLM Tokenizers</title><link href="https://angelalopezcardona.github.io/blog/2025/tokenizer-aligner/" rel="alternate" type="text/html" title="Aligning Tokens between different LLM Tokenizers"/><published>2025-04-18T18:37:00+00:00</published><updated>2025-04-18T18:37:00+00:00</updated><id>https://angelalopezcardona.github.io/blog/2025/tokenizer-aligner</id><content type="html" xml:base="https://angelalopezcardona.github.io/blog/2025/tokenizer-aligner/"><![CDATA[<p>Language models like GPT, Gemini, or Claude have emerged as powerful tools for processing textual representations and modeling complex relationships between input features and output labels. Today, their use has become widespread, with millions of users relying on them in their daily lives.</p> <p>While the internal workings and architectures of these models are beyond the scope of this blog, it‚Äôs important to understand one key concept: the first step in most language models involves converting input text into tokens. These tokens are then mapped to learned vector representations called embeddings. Each model uses its own tokenization algorithm and, during training, learns the embeddings for each token in its vocabulary.</p> <p>Since each model uses a different tokenizer, the same input text can be split into slightly different token sets across models. So what happens when you want to share token-level or embedding-level information between different models? In our experience, you‚Äôve got a problem‚Äîbecause there‚Äôs no straightforward way to align them üòä.</p> <p>After searching for existing solutions, we found that none were available‚Äînot even in the amazing <a href="https://huggingface.co/docs/transformers/en/index"><em>transformers</em></a> library. So, we built our <a href="https://github.com/angelalopezcardona/tokenizer_aligner">own solution</a> and released it on GitHub!</p> <h1 id="tokenizer_aligner">tokenizer_aligner</h1> <p>Python library for mapping tokens between different tokenizers</p> <h2 id="modules">Modules</h2> <p>To map between tokenisers, first, we perform an initial mapping of tokens to the words they belong to in each tokenizer with some properties of <a href="https://huggingface.co/docs/tokenizers/index"><em>FastTokenizers</em></a> from the <a href="https://huggingface.co/docs/transformers/en/index"><em>Transformers</em></a> library. Then, we map words from one tokenizer to the words in the other and finally, we assume that the combination of the tokens that are mapped to a word in one tokenizer correspond to the tokens that are mapped to the word that is mapped to the initial word in the other tokenizer.</p> <p><strong>Token Mapping Example</strong></p> <table> <thead> <tr> <th>¬†</th> <th>Tokenizer 1</th> <th>Tokenizer 2</th> </tr> </thead> <tbody> <tr> <td>Words</td> <td>astrophotography</td> <td>astrophotography</td> </tr> <tr> <td>Tokens</td> <td>[‚Äò_Astro‚Äô, ‚Äòphoto‚Äô, ‚Äògraphy‚Äô]</td> <td>[‚Äòƒä‚Äô, ‚ÄòAst‚Äô, ‚Äòroph‚Äô, ‚Äòot‚Äô, ‚Äòography‚Äô]</td> </tr> <tr> <td>Token indices</td> <td>[22, 23, 24]</td> <td>[23, 24, 25, 26, 271]</td> </tr> <tr> <td>Token IDs</td> <td>[15001, 17720, 16369]</td> <td>[198, 62152, 22761, 354, 5814]</td> </tr> </tbody> </table> <h2 id="installation">Installation</h2> <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>git+https://github.com/angelalopezcardona/tokenizer_aligner.git
</code></pre></div></div>]]></content><author><name></name></author><category term="llm"/><category term="llms"/><summary type="html"><![CDATA[tokenizer aligner python library explained]]></summary></entry><entry><title type="html">Aligning LLMs with human preferences with eye-tracking rewards ‚Äì SYMBIOTIK</title><link href="https://angelalopezcardona.github.io/blog/2024/aligning-llms-with-human-preferences-with-eye-tracking-rewards-symbiotik/" rel="alternate" type="text/html" title="Aligning LLMs with human preferences with eye-tracking rewards ‚Äì SYMBIOTIK"/><published>2024-12-20T00:00:00+00:00</published><updated>2024-12-20T00:00:00+00:00</updated><id>https://angelalopezcardona.github.io/blog/2024/aligning-llms-with-human-preferences-with-eye-tracking-rewards--symbiotik</id><content type="html" xml:base="https://angelalopezcardona.github.io/blog/2024/aligning-llms-with-human-preferences-with-eye-tracking-rewards-symbiotik/"><![CDATA[<p>To better understand the potential of cognitive data in improving machine learning models, specifically Large Language Models (LLMs), we propose GazeReward, an innovative framework for aligning LLMs with human preferences by incorporating eye-tracking (ET) data into reward modeling.LLMs like GPT-4 [1], Llama 3 [2], and others have revolutionized Natural Language Processing (NLP), excelling in diverse tasks but often requiring fine-tuning to meet human expectations. A common approach to achieving human alignment involves leveraging explicit feedback as preference information. Traditional approaches like Reinforcement Learning from Human Feedback (RLHF) [3] have been widely used but they face challenges such as scalability, inconsistent human annotations, and high costs [4]. In addition, obtaining high-quality feedback from human annotators, usually provided after examining a model response, suffers from several caveats. For instance, low inter-annotator agreement can result in inconsistent evaluations of the same model output due to varying interpretations, domain expertise, or biases [5].Incorporating ET data into NLP tasks has proven valuable, as demonstrated by numerous works [6‚Äì14]. Recently, Kiegeland et al. [11] proposed the integration of ET in controlled sentiment generation to create a dataset that can be used in human alignment methods. Our work explores leveraging ET data, a form of implicit feedback that reflects cognitive and perceptual processes during reading and language comprehension, to better model preference modeling. The core idea of GazeReward is to use ET features to enhance the Reward Model (RM), which is central to human alignment techniques.Eye-tracking captures oculomotor behavior‚Äîfixations, saccades, and reading patterns‚Äîthat correlate with attention and information processing. Unlike explicit feedback, ET data is unbiased, temporally precise, and offers rich insights into user preferences [15].To integrate ET signals, we employed state-of-the-art ET prediction models to generate synthetic gaze features from text prompts and responses. These features were combined with text embeddings using two methods: GazeConcat, which concatenates embeddings, and GazeAdd, which adds them element-wise. We conducted experiments using two datasets: OASST1 and HelpSteer2. These datasets include human-annotated preference pairs, where one response is preferred over another. The RM was fine-tuned using open-source LLMs such as Llama 3 and Mistral, with and without ET features. ET prediction models provided features like first fixation duration (FFD) and total reading time (TRT), which were integrated into the RM via a specially designed feature projector. Performance was measured by the RM‚Äôs accuracy in predicting human preferences, using both standard datasets and the RewardBench benchmark.The results demonstrate that incorporating ET features significantly improves RM accuracy, with gains exceeding 20 percent points in some cases. Future work could involve collecting task-specific ET data and extending the framework to other languages and larger models. Additionally, exploring alternative integration methods, such as modifying attention masks, could further enhance performance.GazeReward represents a significant step forward in aligning modern AI systems with human values by integrating cognitive data into reward modeling. This work not only improves the performance of existing models but also opens new avenues for research in multimodal AI alignment. By harnessing the power of gaze, we can make AI more aligned to human needs and expectations. Check the paper for more details!University of Luxembourg, Luxembourg ‚Äì Project Coordinator AEGIS IT Research GmbH, Germany Universit√© Catholique de Louvain, Belgium Telefonica Innovaci√≥n Digital SLU, Spain Sphynx Analytics Ltd, Cyprus¬†Follow us on Twitter!This project is supported by the European Innovation Council (Horizon Europe¬†‚Äì the Framework Programme for Research and Innovation) through the Pathfinder program, ‚ÄúAwareness Inside‚Äù call.</p>]]></content><author><name></name></author></entry></feed>